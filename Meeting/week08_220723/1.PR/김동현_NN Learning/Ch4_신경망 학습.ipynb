{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a883fcf-568b-455b-8ac6-87c3832767fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code : Ch4_신경망 학습\n",
    "- Book : Deep Learning from Scratch 1\n",
    "- Writer : Donghyeon Kim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a163f3-808b-41bf-9d54-5b90a3c2f5e8",
   "metadata": {},
   "source": [
    "#### 4.2. 손실 함수(loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5dd6ef-9802-4f9c-ae2c-5c226c4dbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.2.1. 오차제곱합(SSE)\n",
    "import numpy as np\n",
    "\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7077d0-b4f6-4499-8a8f-c92241798a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답은 '2'\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9aa0629-5b42-4ec1-8c36-d72eea03758c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 : '2'일 확률이 가장 높다고 추정함(0.6)\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2872125d-f520-48a0-aea1-b7faaa1b7ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2 : '7'일 확률이 가장 높다고 추정함(0.6)\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c4c3660-f2e2-4e10-a757-7768ffcedaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.2.2. 교차 엔트로피 오차(CEE)\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "168830fa-21d1-47c7-b891-8b5229ae7f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답은 '2'\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e697c0-748b-4690-9537-5b6a08de03e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 : '2'일 확률이 가장 높다고 추정함(0.6)\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "293bcd42-235d-4ce3-9f76-9c9713412bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2 : '7'일 확률이 가장 높다고 추정함(0.6)\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b14d877-d5b7-4994-b999-f4d847573e17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "## 4.2.3. 미니배치 학습\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape) # (60000, 784)\n",
    "print(t_train.shape) # (60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b8d3d31-afe4-4f3a-a835-0622259bf0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4426255b-8aad-4988-9e16-e9d89d548e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10958, 48681, 11688, 24085, 16082, 25645, 43420, 17240, 39783,\n",
       "       40265])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10) # random choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8d166-ddae-40ee-98c7-6be5b80669a0",
   "metadata": {},
   "source": [
    "#### 4.3. 수치 미분(numerical differentiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "390a75ea-7e2c-4fe5-a51a-6586ffde6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중심 차분\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32363d8d-baac-4c0a-8792-ab2a78077d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 : 2차 함수 미분(quadratic function differentiation)\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c6bfcc9-e0ef-42b7-812b-5ec433f9ec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "print(numerical_diff(function_1, 5)) # 해석적 해 = 0.2\n",
    "print(numerical_diff(function_1, 10)) # 해석적 해 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52ea0a74-7b99-4f76-a931-6ed9dad1b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2 : 편미분(partial differentiation)\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9882638d-b785-45d0-af63-bfe914c7704e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문제 1 : x0 = 3, x1 = 4일 때, x0에 대한 편미분?\n",
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3840d0cb-408d-4c8f-9d48-f13a6b176fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문제 2 : x0 = 3, x1 = 4일 때, x1에 대한 편미분?\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e168e-f6da-46aa-82bf-516de2fe463f",
   "metadata": {},
   "source": [
    "#### 4.4. 기울기(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4549c86d-1296-47a1-8b2f-4d3d82e734fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e189c840-80bf-4f7e-8a59-e06fc486600d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5359e5b-a8e2-495f-91e4-bacb49b71008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# np.zeros_like?\n",
    "# shape가 같고 그 원소가 모두 0인 array임.\n",
    "\n",
    "# example\n",
    "print(np.zeros_like([3.0, 0.0]))\n",
    "print(np.zeros_like([1.0, 2.0, 3.0, 4.0, 5.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4ab2c1b-1642-4997-848f-79d6f1af0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.4.1. Gradient Descent\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8572defa-22f3-40db-bfce-44aedb10bce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문제 : y = x0^2 + x1^2의 최솟값을 Gradient Method로 구하라.\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50a55d7c-6f2a-461b-b901-15ab244d26cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning Rate(학습률)가 너무 큰 예 : lr=10.0\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4da6ce4-0c25-4418-83e0-e66b27b1107d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning Rate(학습률)가 너무 작은 예 : lr=1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4112969a-466c-4998-a0de-e374e7b59b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.4.2. 신경망에서의 기울기\n",
    "import sys, os\n",
    "sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "742c4fe7-1b44-4f3b-80e8-381c8e7130d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15453993 -1.39106706 -0.06065206]\n",
      " [-0.56167546  0.05883544  1.09075052]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W) # 가중치 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06c8e03b-1419-4ee0-9106-ba256926e779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41278396 -0.78168834  0.94528423]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p) # 신경망이 예측한 값(출력값)\n",
    "print(np.argmax(p)) # 최댓값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2974d40-c49a-47ed-a65f-25b04435e418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36115008156849115"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1]) # 정답 레이블\n",
    "net.loss(x, t) # 손실함수의 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a60732b-7f34-4461-94d2-bcc3ca36a890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10752368  0.07435171 -0.18187539]\n",
      " [ 0.16128552  0.11152757 -0.27281309]]\n"
     ]
    }
   ],
   "source": [
    "# Gradient?\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb2a3b-acae-47de-9583-467359ac5af0",
   "metadata": {},
   "source": [
    "#### 4.5. 학습 알고리즘 구현하기(algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89426bcc-9496-4ab7-87f1-ad42189b209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.5.1. 2층 신경망 클래스 구현하기\n",
    "import sys, os\n",
    "sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "371a8be5-f0a0-4f0c-ad9a-9aabd7b04067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09751666666666667, 0.0974\n",
      "train acc, test acc | 0.7847, 0.7899\n",
      "train acc, test acc | 0.87815, 0.8819\n",
      "train acc, test acc | 0.8978166666666667, 0.9015\n",
      "train acc, test acc | 0.90895, 0.9102\n",
      "train acc, test acc | 0.91495, 0.9175\n",
      "train acc, test acc | 0.9196166666666666, 0.9223\n",
      "train acc, test acc | 0.9233666666666667, 0.9261\n",
      "train acc, test acc | 0.9281333333333334, 0.9292\n",
      "train acc, test acc | 0.9314666666666667, 0.9328\n",
      "train acc, test acc | 0.9341333333333334, 0.9353\n",
      "train acc, test acc | 0.9364833333333333, 0.9374\n",
      "train acc, test acc | 0.9391, 0.9388\n",
      "train acc, test acc | 0.94165, 0.9415\n",
      "train acc, test acc | 0.9437, 0.9426\n",
      "train acc, test acc | 0.9449833333333333, 0.9428\n",
      "train acc, test acc | 0.9472666666666667, 0.9462\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAq+UlEQVR4nO3deXiU5b3/8fd3tkw2srMGISLiQhUFqXu1SgtY96p1q7UW9JxiW2s9xVZx6+nPpcd6eg5tta6tHte2ii1VKsVyWosaReoCyiKaAEIMAbLPdv/+mIETQpAJZvIkmc/ruuZinmVmPpOE+c79PPd9P+acQ0REspfP6wAiIuItFQIRkSynQiAikuVUCEREspwKgYhIllMhEBHJchkrBGZ2v5ltMrO3drPdzOxnZrbKzP5pZodnKouIiOxeJlsEDwJTP2H7NGBs6jYT+EUGs4iIyG5krBA45xYDmz9hl9OBX7ukJUCxmQ3LVB4REelawMPXHgHUdFiuTa3b0HlHM5tJstVAfn7+xAMOOKBXAoqIDBSvvfbax865iq62eVkI0uacuwe4B2DSpEmuurra40QiIv2LmX2wu21e9hpaB4zssFyZWiciIr3Iy0IwD/hqqvfQkcBW59wuh4VERCSzMnZoyMweBU4Ays2sFrgBCAI4534JzAemA6uAFuDSTGUREZHdy1ghcM6dv4ftDvhmpl5fRETSo5HFIiJZToVARCTLqRCIiGQ5FQIRkSynQiAikuX6xchiEZGelkg4IvEE0XiCaNwRjSeIxJLL8YQjlnDEU7eO95PLiU7Lnbc74s4RjyeIxBPEoxESkWZcpAWLtkK0BaKtrAmNoy1hDG1ewbDWVfjjbfjjbQQSbQTibfwyeDGRuOOUyHMEEu2MPuV7nDNp5J7fXDepEIhIxrlEgvZIhEg0QiQSpT3STjQSoZ0Qbb48Iu2tBD9eTjzSQjzSTjzahou2sSlvDB/n7IO11DN2wzyItWOxdizehsXbebnwZN4LHEhp61rOargXEnHMxTGXwFych4Ln8ZodyP6RFVwVuw8fyW0+l8BHgmujl1HtDuAE31J+HLyPEAnCJLBU7ssi32OZ24/TfC9xU/BBDAew498vR25kpavkAv9Crg38z45t27d/of121lHBLP/v+V7wyV1+LtNzf017oIjjIws5r/2pnbZFLchLI2fiC4aZWB8j37URKs/PyO9HhUCkP0rEIdYOLg45hcl1DWuhvRFiEYi3E4+0EQ3k0zbkMCKxBL4V80i0NJCItZOIRkjEIjTnVbK+ciqRWIJ9l92Jr30LxKMQb8fiUdYVHkL10POIxBOc8/YsAvEWfIkY5mKYi/NK3ud4quBCotEIP6+7BJ+LJ2/E8ZPgYTedn8TPJRzbxj/DMwl3eht3RM9lbvwMhvMxL4W/tcvbvCl6MQ/Ep7Gf1fJCzs8AiDsjYiEiBPlL82jWhEcSoJnyyHqc+VM3H87vp6osjK+wlFGRCkKbysDnB/ODz4/zBThn1AFMLT6QIc1Ga81KzOfH/H7MfJgZ3xt3FNGiURRvDtK6ditmhs9nGIaZ8V8TToBBw8jfkENire14HJbc/oejphEsLCPno8G49YdjoTwI5kEwFwK5zK86DgI50PwZiMz5v23BXII+P3ft+ElMyMAf0f+x5Liu/kOTzkmflUhArBW2N/0jLRBvxw09hPZYgujaJcTrVhJtbybe1kwi0kLE+Vm1/0xao3FGLf8lpfVLsVgrvlgbvng724Ll/LrqDtpjcS55/98Y0/IGARclQByAlf6xzCq4k/ZYnHtarmJ/9/5Okf4RP4jzo9cB8JfQd9nX99FO2/8cP5wZ0e8BsDB0NUXWTJQAUecnSoA/JyZya+wCgn7jvsBtBC1BwgIkfAGcBViWcziL8qeT4/dxReNdYAHMHwBf8t+aool8WHY8ub4Yn93wCD5/AF8ggN8fxOcP0lQxgbaKQwhbjIpNL+EP5eIPhQnk5BII5RIoGp78IPU5Qq4dXzAX/Pr+ujfM7DXn3KQut6kQyIDnHJhB2zZo2giRpuSHdDR1228KhPKg5lVY+78koq1E21uItrUQb29h9RE30BgPUrL8UYZ+8AzE2rBYG/54K/54hB9WPUpTxHH+pp/yxdb5O710mwtyYOQhnIM7gz/nLP/fdtr+kSvhyPa5ANwYeJBJvvdoIYc2FyJqQTZaBT8JzCDk93E+8xlBHQl/DglfCOcP0Ris4PXiL5IT9DG+tZp8a8cCOfiCOVggTDxcQkvRfuQEfBRFNxH0+wmEcvAHcgiGcgiEcgiFgoT8PnICPkIBH0F/8t9QwEfIn7z5fIb0b59UCFRapW9JxJOHNzreysdCXilsXgMr5ifXRZtx7c3E2ptoPOI7NBVUYauep+zl27FoCxZrwR9txR9v5ZnJ/8P68BjGffAoJ6/9yS4vObPkXlbFKjir+QlmJR7BBzgXJEaIVnK47I0TaGAQF/jXcrq/iTYXopUSYr4hxH1h3tuwhdxwLkvzjqYxPIJEIBeXauJbKJ8rB48lHPQRic/hWX+MYDgff04Bodx8cnNymBf0kRv0Ew6eSG7ITzjoJxzwEfAnO/VdsCPpSXv44R22h+2juve7kKyhFoH0LOcg0gzNddBSD80fQ8X+ULovbF1H4m93EWvdRrx1G4m2bdC+jTUHf4sPy44ld/1LfH7J13d5yp8Nvpm/+SZzQNM/uLnpJgBaXYgWcmglh29FZvG6258jfe/wDf8faSWHZhemlRxayOHXsS+wkVLG+NYzMfgh8dQHtQvkQSiPbfmjCefmURxMUJjjI5ybR0E4RGE4QEFOkIJwgIKcAIXhAPk5yfsFOQH8+pYs/YgODcnecy75Ld0fSJ5EXLWQWNMm2rdsJNpYR7yxjg1DT2Bl+UlEN9dw1t9PJ5Bo3+kp5uZcxoOJ6VS0fcCjvutoJI9Gl0sTuTS5XO6NT+fvic8wjHrO9b9IE7m0+/OJBQuIBwtZn7s/idwyikIJikOOYG4BuTkhcoN+8kLJW24okPrXT17QT14okLy/Y7ufkD91Ik8kC+nQkOwqkUh+Y2/cAP4QifJxbNzWiu+5fyO+ZT2+5o3ktNWRH9vCX/Km8p+hGbS2tPBi+3kESP7htLoQ9QzidyvyuT9eTph2GgIn0+wvpi1UQiSnjHheKe0Fozi5oIzC8AgeDC2mIBygMCf17Toc4Ls5Aeak7hfknE9+yL/jsIiIZJ4KwUCTiCcPxzR9BI0bkx/0OYW4g8/k46YIocfPI1S/nFBbHX6X7Hmy2H8U32j7NpF4goWhF4jhZ6MroTEwjuZgGauDhzCiOMygYYXcm7iPQGE5wcLBFAwqYlBukFNzg1ycF6IoN8ig8Bn6EBfpZ1QI+jPnYMsHsOVDqDqe9licxM+PJXfzOzvttty3P2c9nkdrNM7NgQC57M8mJtMUrMAVDCFeOoZLh45mZEkeNSWLGVmax+TiXMJBfxcvOqFX3pqI9B4Vgv7mo7dg9UKoeYX4hy/jb6mj1V/IRaWP8ub6Jk5yUyi3yWxyJTSHyggUDSe/bBgXlhZTWZLLiNK5VJbkUVmSS36Ofv0iokLQt22thZpXoPZVoidcx4qPY7gX7uOQtfdTa0N5OTaOpYkv8WZsHCGfj0uPGc2hIycwqiyPypI8inKDXr8DEekHVAj6mvVL4W93Ea95BX/jegDayeHcv49iWXQkFUyiYtCxVI2u4vB9SvjyqBLmDBtEKKDj8iKyd1QIvFbzKm7B9dSOn8liJrJ5+duc88FLvBIbw2uJKSxjLL5hh3D4qHJmjCrh8H1KGF6c63VqERlAVAi8tH4p0YfOoC4W5obVb/OXRA7l+YN5s+pRDh9VwpdGlXDtiKLdnLQVEekZKgRe2bScyINnsCka5paKOzn1mIncsE8J+5TmadCTiPQqFQIvNNURuf9UGtqNfy+7jTtnnKIePCLiGX36eGBpvZ/FLZ/n7aLjueMbp6sIiIin9AnUmxo3smbder72eB3Fhefz5MyjKMpTF08R8ZYKQW9p2UzkwdPIqd9MYfC/ePiyzzK4sPP1mkREep86n/eGtm1EHzoTV7+am+0KHpxxDCNL87xOJSICqBBkXqSF6MPnYBvf5LvuKmZd9g32G1zodSoRkR1UCDKsfdFt+Gpf4Xvxb/LVS67gM5VFXkcSEdmJzhFkUFs0zoz3TyAYzeGiiy/js/uWeR1JRGQXKgSZkEgQ/987uXr1RP72QQv/+ZWv8fkDhnidSkSkSzo01NOcI/HH7+JfdAt5q+fz72d8htMOHe51KhGR3VIh6EnO4Z7/Ib7XHmBu7DTGfOEKLvjsPl6nEhH5RCoEPenFW7Elc3kg9kWajvkBV3xujNeJRET2SOcIekrrFhqXPMifYp9j9eE/5JapB3idSEQkLRltEZjZVDN718xWmdnsLrbvY2aLzGypmf3TzKZnMk8mPfLPrXx+6/W8dPAcbj7jEM0gKiL9RsZaBGbmB+YCU4Ba4FUzm+ec63hl9euAJ5xzvzCzg4D5wOhMZcqIZY+z5o0XuX7FdE48YBx3nHs4Pp+KgIj0H5lsEUwGVjnn1jjnIsBjwOmd9nHAoNT9ImB9BvP0vHfmkXj6X/ho9TKOGjWIuRceTtCv0y4i0r9k8hzBCKCmw3It8NlO+9wILDCzK4F84OSunsjMZgIzAfbZp4/0wln5Aomnvs4biX25q+Im7vva0bqSmIj0S15/fT0feNA5VwlMB35jZrtkcs7d45yb5JybVFFR0eshd7FhGYnHLmBFvJKbB93EL7/+OQrDmk5aRPqnTLYI1gEjOyxXptZ1dBkwFcA59w8zCwPlwKYM5vrU6j54h4/jw5mdez33fOMkSvNDXkcSEdlrmWwRvAqMNbMqMwsBXwHmddrnQ+AkADM7EAgDdRnM1CMea57EtLZ/52ffmMrQIl1TQET6t4wVAudcDJgFPA8sJ9k76G0zu9nMTkvtdjUww8yWAY8CX3POuUxl6ik1DS1UFOYwujzf6ygiIp9aRgeUOefmk+wS2nHdnA733wGOyWSGTLji3cuYGDyC3ZzbFhHpV7w+Wdz/xKOMiq6mKFeDskVkYFAh6KZYQw1+ErjikXveWUSkH1Ah6KaG9asAyCnf1+MkIiI9Q4Wgm7alCkHhUM0sKiIDgw50d9O6RDEr40dw4Igqr6OIiPQIFYJuqg5O4r9jRawoHbTnnUVE+gEdGuqm9fXbGDooTCigH52IDAxqEXTT7PfO49Xw0aQGRIuI9Hv6WtsdsXZKE/VYfpnXSUREeowKQTdENn+AD4cV95GpsEVEeoAKQTdsrk12HQ2Vj/Y2iIhID1Ih6IbGjasBKBq2n8dJRER6jgpBN6y1fXgg9kWGVGoMgYgMHCoE3bCUcfx74msMLS7wOoqISI9RIeiGxk0fsk9RAL/PvI4iItJjNI6gG65aezknhScBX/A6iohIj1GLIF3RVkoTm4kUaPppERlYVAjS1F7/QfJOyShvg4iI9DAVgjTV164EIFyhHkMiMrCoEKSp8aPkGILi4boOgYgMLCoEaXovdBA/jp7PEF2HQEQGGBWCNL0ZG8mDdjoVhbleRxER6VHqPpom2/AGhxTl4NMYAhEZYNQiSNMVtbP5Jk96HUNEpMepEKQj0kKJ20K0sNLrJCIiPU6FIA3NdWsA8GkMgYgMQCoEaahPXYcgPFg9hkRk4FEhSEPzxmSLoGS4rkMgIgOPCkEa3sydzJWRWQwdrktUisjAo0KQhhVtpSwMHEdpQdjrKCIiPU7jCNJQUruQYwcNwkxjCERk4FGLIA1f3XQbF7DA6xgiIhmhQrAHrm0bRa6R6CBdh0BEBiYVgj1o3Pg+AP5SjSEQkYEpo4XAzKaa2btmtsrMZu9mn3PN7B0ze9vM/ieTefZGw7rkdQhyK/b1OImISGZk7GSxmfmBucAUoBZ41czmOefe6bDPWOBa4BjnXIOZDc5Unr21fQxB8QiNIRCRgSmTLYLJwCrn3BrnXAR4DDi90z4zgLnOuQYA59ymDObZK9WDpvDl9jkMH65zBCIyMGWyEIwAajos16bWdbQ/sL+Z/d3MlpjZ1K6eyMxmmlm1mVXX1dVlKG7XVjUFeTdnPEV5oV59XRGR3uL1yeIAMBY4ATgf+JWZFXfeyTl3j3NuknNuUkVFRa8GrPrgKaYXrOrV1xQR6U1pFQIz+52ZnWJm3Skc64COx1MqU+s6qgXmOeeizrn3gfdIFoY+4+zNv2Kab4nXMUREMibdD/afAxcAK83sVjMbl8ZjXgXGmlmVmYWArwDzOu3zNMnWAGZWTvJQ0Zo0M2Wca93CIJqIaQyBiAxgaRUC59wLzrkLgcOBtcALZvaSmV1qZsHdPCYGzAKeB5YDTzjn3jazm83stNRuzwP1ZvYOsAi4xjlX/+neUs/ZsmE1AP7S0d4GERHJoLS7j5pZGXARcDGwFHgEOBa4hNS3+s6cc/OB+Z3Wzelw3wHfTd36nIZ1qygB8gZrDIGIDFxpFQIz+z0wDvgNcKpzbkNq0+NmVp2pcF5r2ZQ8SlU2ok+dthAR6VHptgh+5pxb1NUG59ykHszTp/xvyZlc3jaMBcOGex1FRCRj0j1ZfFDHbp1mVmJm/5qZSH3Hh1uitOaPID/c5WkQEZEBId1CMMM5t2X7Qmok8IyMJOpDJq69h3PzXvc6hohIRqVbCPzW4aosqXmEBvZQW+eYuu0pjvQt9zqJiEhGpXuO4DmSJ4bvTi1fnlo3YCVatlBAC7EijSEQkYEt3ULwfZIf/v+SWv4zcG9GEvURm9etpBwIagyBiAxwaRUC51wC+EXqlhW2bFhFOZA/RGMIRGRgS3ccwVjg/wEHAeHt651zA/ZTsnHzR8SdUVapMQQiMrCle7L4AZKtgRhwIvBr4OFMheoL/nfQqYxrf4hhQ4Z6HUVEJKPSLQS5zrmFgDnnPnDO3QickrlY3qvZ3EJpYT7hUMYu4iYi0iek+ynXnpqCeqWZzSI5nXRB5mJ57wvv386+4SrgZK+jiIhkVLotgm8DecC3gIkkJ5+7JFOhPOccx7S8wIGB9V4nERHJuD22CFKDx85zzn0PaAIuzXgqj8Ua68ijjUTRKK+jiIhk3B5bBM65OMnpprNG/brkpSmDZSoEIjLwpXuOYKmZzQOeBJq3r3TO/S4jqTy2df0qhgAFQ/fzOoqISMalWwjCQD3w+Q7rHDAgC8HmxmY2uFLKKlUIRGTgS3dk8YA/L9DRS3mf54LIPrw7eLDXUUREMi7dkcUPkGwB7MQ59/UeT9QH1Da0Mqwol6A/3U5VIiL9V7qHhv7Q4X4YOBMYsH0rv7JmNhNDB7LzkTARkYEp3UNDv+24bGaPAn/LSCKvOceh7a/Rkr+P10lERHrF3h77GAsMyAPo7Vs3kEOERLEKgYhkh3TPETSy8zmCj0heo2DAqa9dxXAgVF7ldRQRkV6R7qGhwkwH6Su2bVjNcGDQ0AE7w7aIyE7SOjRkZmeaWVGH5WIzOyNjqTy0sdVYmtiPcl2HQESyRLrnCG5wzm3dvuCc2wLckJFEHns5dCTnxG5hSHmZ11FERHpFuoWgq/0G5ET9NQ2tDC/Oxe8zr6OIiPSKdD/Mq83sTmBuavmbwGuZieStK1fP5L3woSQvxCYiMvCl2yK4EogAjwOPAW0ki8HAkkgwOraGgnDQ6yQiIr0m3V5DzcDsDGfxXGvDenKJgcYQiEgWSbfX0J/NrLjDcomZPZ+xVB6pr30PgJyK0d4GERHpRekeGipP9RQCwDnXwAAcWbxtw2oACnUdAhHJIukWgoSZ7TheYmaj6WI20v6uNjaIP8Q/y+DKMV5HERHpNen2Gvoh8Dcz+ytgwHHAzIyl8ki17xAedFexoqTE6ygiIr0m3ZPFz5nZJJIf/kuBp4HWDObyxIb6LVSW5OLTGAIRySLpniz+BrAQuBr4HvAb4MY0HjfVzN41s1VmttteR2Z2tpm5VLHxzLXvX8oNiV94GUFEpNele47g28ARwAfOuROBw4Atn/QAM/OTHIA2DTgION/MDupiv8LU87+cfuwMSMSpSGyC/HJPY4iI9LZ0C0Gbc64NwMxynHMrgHF7eMxkYJVzbo1zLkJyINrpXex3C3AbyUFqnmn6uIYgcSgZ5WUMEZFel24hqE2NI3ga+LOZPQN8sIfHjABqOj5Hat0OZnY4MNI598dPeiIzm2lm1WZWXVdXl2bk7vm4diUAOboOgYhkmXRPFp+ZunujmS0CioDnPs0Lm5kPuBP4Whqvfw9wD8CkSZMy0m21MTWGoGiYuo6KSHbp9gyizrm/prnrOmBkh+XK1LrtCoHxwItmBjAUmGdmpznnqrub69N6n+EsiU3n7JEaTCYi2WVvr1mcjleBsWZWZWYh4CvAvO0bnXNbnXPlzrnRzrnRwBLAkyIAsDQxhp/6LqFkUNZcjE1EBMhgIXDOxYBZwPPAcuAJ59zbZnazmZ2WqdfdWy11axlT7CfVOhERyRoZvbiMc24+ML/Tujm72feETGbZk6tqv8P7uZ8BpngZQ0Sk12Xy0FC/4eJRyuMfEy2s9DqKiEivUyEAGjd9QMASmK5DICJZSIUA+LhmFQA5Feo6KiLZR4UAaNqYHENQPGJfj5OIiPQ+FQJgRWB/bo5ezJBKjSEQkeyjQgC82T6Mp4KnUlSQ73UUEZFep0IABD9ayoSiZq9jiIh4IqPjCPqLKzbeyOr8w4CzvI4iItLrsr5F4GLtlCXqiRaO3PPOIiIDUNYXgs0b1uI3h69EYwhEJDupEKxLXocgb7C6jopIdsr6QtC8cQ0ARcPHepxERMQbWV8I3siZyBWR7zB0pFoEIpKdsr4QvNsyiFdzjyU/N+x1FBERT2R999Hy9X/h+MIir2OIiHgm61sEX63/T85N/MnrGCIinsnqQpCItFGW2ExM1yEQkSyW1YXg4/Wr8ZnDVzra6ygiIp7J6kLQsD55HYL8IeoxJCLZK6sLQfNHyesQlAzX9NMikr2yutfQK/knclO7n8dHVHkdRUTEM1ndIli9zceGgoMJ54S8jiIi4pmsbhHsV/tbCvMrgJO9jiIi4pmsbhGcvfXXfMG95HUMERFPZW0hiLU1U04DsUG6DoGIZLesLQR1tcmuo4HSUR4nERHxVtYWgu1jCPKGjPE4iYiIt7K2ELRueh+AshG6DoGIZLesLQR/LTyFI9vnMniEDg2JSHbL2kJQs6Udf9FwgoGs7kErIpK94wgm19xHVXgY8Hmvo4iIeCprWwRfbHqaI3jH6xgiIp7LykLQ3rKNUraRKNIYAhGRrCwEdbXJWUcDZaO9DSIi0gdktBCY2VQze9fMVpnZ7C62f9fM3jGzf5rZQjPrlS48DeuSYwgKdB0CEZHMFQIz8wNzgWnAQcD5ZnZQp92WApOcc4cATwG3ZypPR00NG4k6P2WVGkMgIpLJFsFkYJVzbo1zLgI8BpzecQfn3CLnXEtqcQnQKxcPXpx3MuNjv6Zi6D698XIiIn1aJgvBCKCmw3Jtat3uXAb8qasNZjbTzKrNrLquru5TB6ttaGVocT5+f1aeIhER2Umf+CQ0s4uAScAdXW13zt3jnJvknJtUUVHxqV9v6gc/4evBBZ/6eUREBoJMFoJ1QMf+mZWpdTsxs5OBHwKnOefaM5hnh2Pa/so43y5RRESyUiYLwavAWDOrMrMQ8BVgXscdzOww4G6SRWBTBrPs0Nq4hWIaSRTp/ICICGSwEDjnYsAs4HlgOfCEc+5tM7vZzE5L7XYHUAA8aWZvmNm83Txdj9lU8x4AQY0hEBEBMjzXkHNuPjC/07o5He73+sWCt25IDiYrGKrrEIiIQBZOOrd5WzM1iQrKK/fzOoqI7EY0GqW2tpa2tjavo/Q74XCYyspKgsFg2o/JukLw99DRXJ74L1YM6ZUhCyKyF2prayksLGT06NGYmddx+g3nHPX19dTW1lJVVZX24/pE99HeVNvQSmVJrv64RPqwtrY2ysrK9P+0m8yMsrKybreksq5FcNGH11EX3hc4wesoIvIJVAT2zt783LKuRfCZyDKGhpq9jiEi0mdkVSFo3PIxg2jGFek6xSKye1u2bOHnP//5Xj12+vTpbNmypWcDZVhWFYJNNcnpp0Plo70NIiJ92icVglgs9omPnT9/PsXFxRlIlTlZdY5g24ZkISgcojEEIv3FTc++zTvrt/Xocx40fBA3nHrwbrfPnj2b1atXM2HCBKZMmcIpp5zC9ddfT0lJCStWrOC9997jjDPOoKamhra2Nr797W8zc+ZMAEaPHk11dTVNTU1MmzaNY489lpdeeokRI0bwzDPPkJubu9NrPfvss/zoRz8iEolQVlbGI488wpAhQ2hqauLKK6+kuroaM+OGG27g7LPP5rnnnuMHP/gB8Xic8vJyFi5c+Kl/HllVCDa2wMuJA9h/n/29jiIifditt97KW2+9xRtvvAHAiy++yOuvv85bb721o1vm/fffT2lpKa2trRxxxBGcffbZlJWV7fQ8K1eu5NFHH+VXv/oV5557Lr/97W+56KKLdtrn2GOPZcmSJZgZ9957L7fffjv/8R//wS233EJRURFvvvkmAA0NDdTV1TFjxgwWL15MVVUVmzdv7pH3m1WF4GX/YTzBTbxVNsTrKCKSpk/65t6bJk+evFPf/J/97Gf8/ve/B6CmpoaVK1fuUgiqqqqYMGECABMnTmTt2rW7PG9tbS3nnXceGzZsIBKJ7HiNF154gccee2zHfiUlJTz77LMcf/zxO/YpLS3tkfeWVecIaja3UlmSp25pItJt+fn5O+6/+OKLvPDCC/zjH/9g2bJlHHbYYV323c/Jydlx3+/3d3l+4corr2TWrFm8+eab3H333Z6Mps6qQnDVh7P4fuJer2OISB9XWFhIY2Pjbrdv3bqVkpIS8vLyWLFiBUuWLNnr19q6dSsjRiSv2fXQQw/tWD9lyhTmzp27Y7mhoYEjjzySxYsX8/777wP02KGhrCkELpFgVGwt+eH0598QkexUVlbGMcccw/jx47nmmmt22T516lRisRgHHnggs2fP5sgjj9zr17rxxhs555xzmDhxIuXl5TvWX3fddTQ0NDB+/HgOPfRQFi1aREVFBffccw9nnXUWhx56KOedd95ev25H5pzrkSfqLZMmTXLV1dXdftyWjzdS/N/7s2TsdznywhsykExEesry5cs58MADvY7Rb3X18zOz15xzk7raP2taBHW1KwHIqdjX4yQiIn1L1hSCbR8lr0MwaKgKgYhIR1nTfbQ2kk9N/GhOHDnO6ygiIn1K1hSC8UdNZVnlkRSVlO95ZxGRLJI1hWBMRQFjKgq8jiEi0udkzTkCERHpmgqBiEgnn2YaaoC77rqLlpaWHkyUWSoEIiKdZFshyJpzBCLSjz1wyq7rDj4DJs+ASAs8cs6u2ydcAIddCM318MRXd9526R8/8eU6T0N9xx13cMcdd/DEE0/Q3t7OmWeeyU033URzczPnnnsutbW1xONxrr/+ejZu3Mj69es58cQTKS8vZ9GiRTs9980338yzzz5La2srRx99NHfffTdmxqpVq7jiiiuoq6vD7/fz5JNPMmbMGG677TYefvhhfD4f06ZN49Zbb+3mD2/PVAhERDrpPA31ggULWLlyJa+88grOOU477TQWL15MXV0dw4cP549/TBaWrVu3UlRUxJ133smiRYt2mjJiu1mzZjFnzhwALr74Yv7whz9w6qmncuGFFzJ79mzOPPNM2traSCQS/OlPf+KZZ57h5ZdfJi8vr8fmFupMhUBE+r5P+gYfyvvk7flle2wB7MmCBQtYsGABhx12GABNTU2sXLmS4447jquvvprvf//7fOlLX+K4447b43MtWrSI22+/nZaWFjZv3szBBx/MCSecwLp16zjzzDMBCIfDQHIq6ksvvZS8vDyg56ad7kyFQERkD5xzXHvttVx++eW7bHv99deZP38+1113HSeddNKOb/tdaWtr41//9V+prq5m5MiR3HjjjZ5MO92ZThaLiHTSeRrqL37xi9x///00NTUBsG7dOjZt2sT69evJy8vjoosu4pprruH111/v8vHbbf/QLy8vp6mpiaeeemrH/pWVlTz99NMAtLe309LSwpQpU3jggQd2nHjWoSERkV7ScRrqadOmcccdd7B8+XKOOuooAAoKCnj44YdZtWoV11xzDT6fj2AwyC9+8QsAZs6cydSpUxk+fPhOJ4uLi4uZMWMG48ePZ+jQoRxxxBE7tv3mN7/h8ssvZ86cOQSDQZ588kmmTp3KG2+8waRJkwiFQkyfPp0f//jHPf5+s2YaahHpPzQN9aejaahFRKRbVAhERLKcCoGI9En97bB1X7E3PzcVAhHpc8LhMPX19SoG3eSco76+fsc4hHSp15CI9DmVlZXU1tZSV1fndZR+JxwOU1lZ2a3HqBCISJ8TDAapqqryOkbWyOihITObambvmtkqM5vdxfYcM3s8tf1lMxudyTwiIrKrjBUCM/MDc4FpwEHA+WZ2UKfdLgManHP7AT8FbstUHhER6VomWwSTgVXOuTXOuQjwGHB6p31OBx5K3X8KOMnMLIOZRESkk0yeIxgB1HRYrgU+u7t9nHMxM9sKlAEfd9zJzGYCM1OLTWb27l5mKu/83H2EcnWPcnVfX82mXN3zaXKN2t2GfnGy2Dl3D3DPp30eM6ve3RBrLylX9yhX9/XVbMrVPZnKlclDQ+uAkR2WK1PrutzHzAJAEVCfwUwiItJJJgvBq8BYM6sysxDwFWBep33mAZek7n8Z+IvTCBIRkV6VsUNDqWP+s4DnAT9wv3PubTO7Gah2zs0D7gN+Y2argM0ki0UmferDSxmiXN2jXN3XV7MpV/dkJFe/m4ZaRER6luYaEhHJcioEIiJZLmsKwZ6mu/CCmY00s0Vm9o6ZvW1m3/Y6U0dm5jezpWb2B6+zbGdmxWb2lJmtMLPlZnaU15kAzOyq1O/wLTN71My6N/1jz+W438w2mdlbHdaVmtmfzWxl6t+SPpLrjtTv8Z9m9nszK+4LuTpsu9rMnJmV95VcZnZl6mf2tpnd3lOvlxWFIM3pLrwQA652zh0EHAl8s4/k2u7bwHKvQ3Tyn8BzzrkDgEPpA/nMbATwLWCSc248yc4Rme74sDsPAlM7rZsNLHTOjQUWppZ724PsmuvPwHjn3CHAe8C1vR2KrnNhZiOBLwAf9naglAfplMvMTiQ5G8OhzrmDgZ/01ItlRSEgvekuep1zboNz7vXU/UaSH2ojvE2VZGaVwCnAvV5n2c7MioDjSfY2wzkXcc5t8TTU/wkAuanxMHnAei9COOcWk+yB11HHqVweAs7ozUzQdS7n3ALnXCy1uITkWCPPc6X8FPg3wJPeNLvJ9S/Arc659tQ+m3rq9bKlEHQ13UWf+MDdLjXz6mHAyx5H2e4ukv8REh7n6KgKqAMeSB2yutfM8r0O5ZxbR/Lb2YfABmCrc26Bt6l2MsQ5tyF1/yNgiJdhduPrwJ+8DgFgZqcD65xzy7zO0sn+wHGpmZr/amZH9NQTZ0sh6NPMrAD4LfAd59y2PpDnS8Am59xrXmfpJAAcDvzCOXcY0Iw3hzl2kjrmfjrJQjUcyDezi7xN1bXUgM0+1WfczH5I8jDpI30gSx7wA2CO11m6EABKSR5GvgZ4oqcm6cyWQpDOdBeeMLMgySLwiHPud17nSTkGOM3M1pI8jPZ5M3vY20hAsiVX65zb3mp6imRh8NrJwPvOuTrnXBT4HXC0x5k62mhmwwBS//bYIYVPy8y+BnwJuLCPzCowhmRBX5b6+68EXjezoZ6mSqoFfueSXiHZWu+RE9nZUgjSme6i16Wq+X3AcufcnV7n2c45d61zrtI5N5rkz+ovzjnPv+E65z4CasxsXGrVScA7Hkba7kPgSDPLS/1OT6IPnMTuoONULpcAz3iYZQczm0ry8ONpzrkWr/MAOOfedM4Nds6NTv391wKHp/72vPY0cCKAme0PhOihGVKzohCkTkhtn+5iOfCEc+5tb1MByW/eF5P8xv1G6jbd61B93JXAI2b2T2AC8GNv40CqhfIU8DrwJsn/V55MUWBmjwL/AMaZWa2ZXQbcCkwxs5UkWy+39pFc/w0UAn9O/e3/so/k8txuct0P7JvqUvoYcElPtaI0xYSISJbLihaBiIjsngqBiEiWUyEQEclyKgQiIllOhUBEJMupEIhkmJmd0JdmcBXpTIVARCTLqRCIpJjZRWb2Smpw092p6zE0mdlPU/O/LzSzitS+E8xsSYe59EtS6/czsxfMbJmZvW5mY1JPX9DhOgqPbJ8jxsxuteT1KP5pZj02rbBId6gQiABmdiBwHnCMc24CEAcuBPKB6tT8738Fbkg95NfA91Nz6b/ZYf0jwFzn3KEk5xvaPuvnYcB3SF4PY1/gGDMrA84EDk49z48y+R5FdkeFQCTpJGAi8KqZvZFa3pfkxF6Pp/Z5GDg2dV2EYufcX1PrHwKON7NCYIRz7vcAzrm2DnPovOKcq3XOJYA3gNHAVqANuM/MzgL6xHw7kn1UCESSDHjIOTchdRvnnLuxi/32dk6W9g7340AgNQfWZJLzFH0JeG4vn1vkU1EhEElaCHzZzAbDjuv8jiL5f+TLqX0uAP7mnNsKNJjZcan1FwN/TV1lrtbMzkg9R05qfvsupa5DUeScmw9cRfLSmyK9LuB1AJG+wDn3jpldBywwMx8QBb5J8uI3k1PbNpE8jwDJ6Zx/mfqgXwNcmlp/MXC3md2ceo5zPuFlC4FnLHmhewO+28NvSyQtmn1U5BOYWZNzrsDrHCKZpENDIiJZTi0CEZEspxaBiEiWUyEQEclyKgQiIllOhUBEJMupEIiIZLn/D0d4BZbkz7fAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 4.5.2. 미니배치 학습 구현하기\n",
    "import sys, os\n",
    "sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432cae5-3724-4c54-8281-a894a8f0a66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
